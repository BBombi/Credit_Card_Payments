{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMnx3pHIuojl"
   },
   "outputs": [],
   "source": [
    "# Loading Libraries\n",
    "import math as mt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import scipy\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from plotnine import *\n",
    "%matplotlib inline\n",
    "import pandas_profiling\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Estimators\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#Model metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Cross Validation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.00000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15000.500000</td>\n",
       "      <td>167484.322667</td>\n",
       "      <td>1.603733</td>\n",
       "      <td>1.853133</td>\n",
       "      <td>1.551867</td>\n",
       "      <td>35.485500</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>-0.133767</td>\n",
       "      <td>-0.166200</td>\n",
       "      <td>-0.220667</td>\n",
       "      <td>...</td>\n",
       "      <td>43262.948967</td>\n",
       "      <td>40311.400967</td>\n",
       "      <td>38871.760400</td>\n",
       "      <td>5663.580500</td>\n",
       "      <td>5.921163e+03</td>\n",
       "      <td>5225.68150</td>\n",
       "      <td>4826.076867</td>\n",
       "      <td>4799.387633</td>\n",
       "      <td>5215.502567</td>\n",
       "      <td>0.221200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8660.398374</td>\n",
       "      <td>129747.661567</td>\n",
       "      <td>0.489129</td>\n",
       "      <td>0.790349</td>\n",
       "      <td>0.521970</td>\n",
       "      <td>9.217904</td>\n",
       "      <td>1.123802</td>\n",
       "      <td>1.197186</td>\n",
       "      <td>1.196868</td>\n",
       "      <td>1.169139</td>\n",
       "      <td>...</td>\n",
       "      <td>64332.856134</td>\n",
       "      <td>60797.155770</td>\n",
       "      <td>59554.107537</td>\n",
       "      <td>16563.280354</td>\n",
       "      <td>2.304087e+04</td>\n",
       "      <td>17606.96147</td>\n",
       "      <td>15666.159744</td>\n",
       "      <td>15278.305679</td>\n",
       "      <td>17777.465775</td>\n",
       "      <td>0.415062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-170000.000000</td>\n",
       "      <td>-81334.000000</td>\n",
       "      <td>-339603.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7500.750000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2326.750000</td>\n",
       "      <td>1763.000000</td>\n",
       "      <td>1256.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>8.330000e+02</td>\n",
       "      <td>390.00000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>252.500000</td>\n",
       "      <td>117.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15000.500000</td>\n",
       "      <td>140000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19052.000000</td>\n",
       "      <td>18104.500000</td>\n",
       "      <td>17071.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2.009000e+03</td>\n",
       "      <td>1800.00000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22500.250000</td>\n",
       "      <td>240000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54506.000000</td>\n",
       "      <td>50190.500000</td>\n",
       "      <td>49198.250000</td>\n",
       "      <td>5006.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>4505.00000</td>\n",
       "      <td>4013.250000</td>\n",
       "      <td>4031.500000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>891586.000000</td>\n",
       "      <td>927171.000000</td>\n",
       "      <td>961664.000000</td>\n",
       "      <td>873552.000000</td>\n",
       "      <td>1.684259e+06</td>\n",
       "      <td>896040.00000</td>\n",
       "      <td>621000.000000</td>\n",
       "      <td>426529.000000</td>\n",
       "      <td>528666.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
       "count  30000.000000    30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean   15000.500000   167484.322667      1.603733      1.853133      1.551867   \n",
       "std     8660.398374   129747.661567      0.489129      0.790349      0.521970   \n",
       "min        1.000000    10000.000000      1.000000      0.000000      0.000000   \n",
       "25%     7500.750000    50000.000000      1.000000      1.000000      1.000000   \n",
       "50%    15000.500000   140000.000000      2.000000      2.000000      2.000000   \n",
       "75%    22500.250000   240000.000000      2.000000      2.000000      2.000000   \n",
       "max    30000.000000  1000000.000000      2.000000      6.000000      3.000000   \n",
       "\n",
       "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      35.485500     -0.016700     -0.133767     -0.166200     -0.220667   \n",
       "std        9.217904      1.123802      1.197186      1.196868      1.169139   \n",
       "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
       "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
       "\n",
       "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
       "count  ...   30000.000000   30000.000000   30000.000000   30000.000000   \n",
       "mean   ...   43262.948967   40311.400967   38871.760400    5663.580500   \n",
       "std    ...   64332.856134   60797.155770   59554.107537   16563.280354   \n",
       "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
       "25%    ...    2326.750000    1763.000000    1256.000000    1000.000000   \n",
       "50%    ...   19052.000000   18104.500000   17071.000000    2100.000000   \n",
       "75%    ...   54506.000000   50190.500000   49198.250000    5006.000000   \n",
       "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
       "\n",
       "           PAY_AMT2      PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
       "count  3.000000e+04   30000.00000   30000.000000   30000.000000   \n",
       "mean   5.921163e+03    5225.68150    4826.076867    4799.387633   \n",
       "std    2.304087e+04   17606.96147   15666.159744   15278.305679   \n",
       "min    0.000000e+00       0.00000       0.000000       0.000000   \n",
       "25%    8.330000e+02     390.00000     296.000000     252.500000   \n",
       "50%    2.009000e+03    1800.00000    1500.000000    1500.000000   \n",
       "75%    5.000000e+03    4505.00000    4013.250000    4031.500000   \n",
       "max    1.684259e+06  896040.00000  621000.000000  426529.000000   \n",
       "\n",
       "            PAY_AMT6  default payment next month  \n",
       "count   30000.000000                30000.000000  \n",
       "mean     5215.502567                    0.221200  \n",
       "std     17777.465775                    0.415062  \n",
       "min         0.000000                    0.000000  \n",
       "25%       117.750000                    0.000000  \n",
       "50%      1500.000000                    0.000000  \n",
       "75%      4000.000000                    0.000000  \n",
       "max    528666.000000                    1.000000  \n",
       "\n",
       "[8 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_csv('default of credit card clients.csv', header =1)\n",
    "credit.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average value for the amount of credit card limit is 167,484. The standard deviation is unusually large, max value being 1M.\n",
    "\n",
    "Education level is mostly graduate school and university.\n",
    "\n",
    "Most of the clients are either marrined or single (less frequent the other status).\n",
    "\n",
    "Average age is 35.5 years, with a standard deviation of 9.2.\n",
    "\n",
    "As the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis).\n",
    "\n",
    "### **\"New\" values:**\n",
    "Â· *EDUCATION:* 1 = graduate school; 2 = university; 3 = high school; 0, 4, 5, 6 = others.\n",
    "\n",
    "Â· *MARRIAGE:* 1 = married; 2 = single; 3 = divorce; 0=others.\n",
    "\n",
    "Â· *PAY_0 to PAY_6:* -2 = No consumption; -1= Paid in full; 0 = The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.columns = ['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0',\n",
    "       'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
    "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    "       'default_payment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_payment</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percent</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         default_payment  PAY_6  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  \\\n",
       "Total                0.0    0.0        0.0  0.0        0.0       0.0  0.0   \n",
       "Percent              0.0    0.0        0.0  0.0        0.0       0.0  0.0   \n",
       "\n",
       "         PAY_0  PAY_2  PAY_3  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  \\\n",
       "Total      0.0    0.0    0.0  ...        0.0        0.0        0.0        0.0   \n",
       "Percent    0.0    0.0    0.0  ...        0.0        0.0        0.0        0.0   \n",
       "\n",
       "         PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5   ID  \n",
       "Total         0.0       0.0       0.0       0.0       0.0  0.0  \n",
       "Percent       0.0       0.0       0.0       0.0       0.0  0.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking for NA's\n",
    "total = credit.isnull().sum().sort_values(ascending = False)\n",
    "percent = (credit.isnull().sum()/credit.isnull().count()*100).sort_values(ascending = False)\n",
    "pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no NA's, so there's no need to treat them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    29965.000000\n",
       "mean         0.221258\n",
       "std          0.415101\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: default_payment, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_credit = credit.drop_duplicates(subset= ('LIMIT_BAL',\n",
    " 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3',\n",
    " 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
    " 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2',\n",
    " 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    " 'default_payment'), keep='first').reset_index(drop=True)\n",
    "new_credit.default_payment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've removed all duplicates, but there were only 35 rows duplicated out of 30K rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_credit.head()\n",
    "# cols = [col for col in new_credit.columns[6:12]]\n",
    "\n",
    "# for col in cols:\n",
    "#    print(col)\n",
    "#    print(len(new_credit[new_credit[col]==-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in cols:\n",
    "#    new_credit = new_credit[new_credit[col] > -2]\n",
    "    \n",
    "# print(len(new_credit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_credit.head()\n",
    "# cols2 = [col for col in new_credit.columns[12:18]]\n",
    "\n",
    "#for col in cols2:\n",
    "#    print(col)\n",
    "#    print(len(new_credit[new_credit[col]<0]),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_credit.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â· According to the correlation matrix plotted above, there are a high correlation between all \"BILL_AMT\" features, and the one that helps our model to predict the Default Payment better is the \"BILL_AMT1\".\n",
    "\n",
    "Â· Features \"PAY_AMT\" are quite skewed, that means that they're not following a normal distribution. Many predictive models depend on normality assumptions.\n",
    "\n",
    "Â· Again, we can see that the Default Payment is highly biased to \"No\". That means that we'll need to upsample or downsample the train set in order to aboid the model to predict \"No\" by default.\n",
    "\n",
    "Â· I assume that some Feature Engineering will be needed, as it doesn't seems to be a strong correlation between any independent variables against the dependant one.\n",
    "\n",
    "Â· For all the \"PAY_n\" features, there's \"-2\", \"-1\" and \"0\", that actually means preatty much the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_credit.PAY_0.replace([-2,0], [-1,-1], inplace=True)\n",
    "# new_credit.PAY_2.replace([-2,0], [-1,-1], inplace=True)\n",
    "# new_credit.PAY_3.replace([-2,0], [-1,-1], inplace=True)\n",
    "# new_credit.PAY_4.replace([-2,0], [-1,-1], inplace=True)\n",
    "# new_credit.PAY_5.replace([-2,0], [-1,-1], inplace=True)\n",
    "# new_credit.PAY_6.replace([-2,0], [-1,-1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some functions to Normalize and Standarize a vector of data\n",
    "#input = array of values to be standardized\n",
    "def standardize(lista = np.random.normal(loc=10, scale=20, size=1000)):\n",
    "    stand_lista = np.ones(len(lista))\n",
    "    for i in range(len(lista)):\n",
    "        stand_lista[i] = (lista[i] - lista.mean())/(lista.var()**(1/2))\n",
    "    return(stand_lista)\n",
    "\n",
    "#input = array of values to be normalized\n",
    "def normalize(lista = np.random.normal(loc=10, scale=20, size=1000)):\n",
    "    norm_lista = np.ones(len(lista))\n",
    "    for i in range(len(lista)):\n",
    "        norm_lista[i] = (lista[i] - lista.min())/(lista.max() - lista.min())\n",
    "    return(norm_lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 20))\n",
    "stats.probplot(new_credit.PAY_AMT1, plot=fig.add_subplot(131))\n",
    "stats.probplot(new_credit.PAY_AMT2, plot=fig.add_subplot(132))\n",
    "stats.probplot(new_credit.PAY_AMT3, plot=fig.add_subplot(133))\n",
    "stats.probplot(new_credit.PAY_AMT4, plot=fig.add_subplot(231))\n",
    "stats.probplot(new_credit.PAY_AMT5, plot=fig.add_subplot(232))\n",
    "stats.probplot(new_credit.PAY_AMT6, plot=fig.add_subplot(233))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 20))\n",
    "stats.probplot(new_credit.AGE, plot=fig.add_subplot(121))\n",
    "stats.probplot(new_credit.LIMIT_BAL, plot=fig.add_subplot(122))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "plt.title('Amount of credit limit - Density Plot')\n",
    "sns.distplot(new_credit['LIMIT_BAL'],kde=True,bins=200, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_credit.LIMIT_BAL = standardize(new_credit.LIMIT_BAL)\n",
    "\n",
    "#Plotting after Standardize\n",
    "plt.figure(figsize = (14,10))\n",
    "plt.title('Amount of credit limit - Density Plot')\n",
    "sns.distplot(new_credit['LIMIT_BAL'],kde=True,bins=200, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_credit['LIMIT_BAL'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_credit.SEX.replace([1,2],[\"Male\",\"Female\"],inplace=True)\n",
    "\n",
    "# new_credit.EDUCATION.replace([0,1,2,3,4,5,6],\n",
    "#                         [\"Others\",\"Graduate school\",\"University\",\"High school\",\"Others\",\"Others\",\"Others\"],\n",
    "#                         inplace=True)\n",
    "\n",
    "new_credit.EDUCATION.replace([0,1,2,3,4,5,6], [4,1,2,3,4,4,4], inplace=True)\n",
    "\n",
    "# new_credit.MARRIAGE.replace([1,2,3,0], [\"Married\",\"Single\",\"Divorced\",\"Others\"],inplace=True)\n",
    "\n",
    "new_credit.default_payment.replace([1,0],[\"Yes\",\"No\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(new_credit, aes(x=\"default_payment\")) \n",
    " + geom_bar(color=\"darkblue\", fill=\"lightblue\") \n",
    " + facet_wrap(\"EDUCATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(new_credit, aes(x = \"AGE\", fill = \"default_payment\")) \n",
    " + geom_bar() \n",
    " + facet_wrap(\"EDUCATION\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(new_credit, aes(x = \"AGE\", fill = \"default_payment\")) \n",
    " + geom_bar() \n",
    " + facet_wrap(\"MARRIAGE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(new_credit, aes(x = \"AGE\", fill = \"default_payment\")) \n",
    " + geom_bar() \n",
    " + facet_wrap(\"SEX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerpocessing: Standardize the data\n",
    "new_credit.LIMIT_BAL = standardize(new_credit.LIMIT_BAL)\n",
    "new_credit.AGE = standardize(new_credit.AGE)\n",
    "new_credit.PAY_0 = standardize(new_credit.PAY_0)\n",
    "new_credit.PAY_AMT1 = standardize(new_credit.PAY_AMT1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = new_credit.iloc[:,[1,2,3,4,5,6,12]]\n",
    "depVar = new_credit.iloc[:,24]\n",
    "\n",
    "# Splitting Training and Test Sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, depVar, test_size=0.3)\n",
    "y_test_count = len(y_test.index)\n",
    "print('The number of observations in the Y training set are:',str(y_test_count))\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model\n",
    "modelRF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
    "            oob_score=False, random_state=2018, verbose=False,\n",
    "            warm_start=False)\n",
    "\n",
    "modelRF.fit(X_train,y_train)\n",
    "print(cross_val_score(modelRF, X_train, y_train),\"\\n\", modelRF.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF, X_test, y_test),\"\\n\", modelRF.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRF.score(new_credit.iloc[:,[1,2,3,4,5,6,12]],new_credit.iloc[:,[24]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughs\n",
    "In our dataset, almost 20% of clients default. That mena that an *Accuracy* close to 80% it's actually really bad, as I don't need any algorithm to set all the predictions to \"0\" (*Won't default*) to get an 80% *Accurracy*. In this sense, *Accuracy* can be a misleading metric of the quality of our model.\n",
    "\n",
    "A better metric is the **f1-score**, which takes into account the false positives, the false negatives etc.\n",
    "\n",
    "So we define the **precision** as *TP/(TP+FP)* and **recall** as *TP/(TP+FN)* and we have *F1 = 2 (Prec Rec) / (Prec + Rec)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = modelRF.predict(X_test)\n",
    "print(\"F1 Score:\", f1_score(y_test, predictions, average='weighted'))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions, average='weighted'))\n",
    "print('Precision:', precision_score(y_test, predictions, average='weighted'))\n",
    "print('\\n clasification report:\\n', classification_report(y_test,predictions))\n",
    "print('\\n confussion matrix:\\n',confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive (TP): we predict \"Yes\" (1), and the true label is \"Yes\".\n",
    "TP = np.sum(np.logical_and(predictions == \"Yes\", y_test == \"Yes\"))\n",
    " \n",
    "# True Negative (TN): we predict \"No\" (0), and the true label is \"No\".\n",
    "TN = np.sum(np.logical_and(predictions == \"No\", y_test == \"No\"))\n",
    " \n",
    "# False Positive (FP): we predict \"Yes\" (1), but the true label is \"No\".\n",
    "FP = np.sum(np.logical_and(predictions == \"Yes\", y_test == \"No\"))\n",
    " \n",
    "# False Negative (FN): we predict \"No\" (0), but the true label is \"Yes\".\n",
    "FN = np.sum(np.logical_and(predictions == \"No\", y_test == \"Yes\"))\n",
    " \n",
    "print('TP: {}, FP: {}, TN: {}, FN: {}'.format(TP,FP,TN,FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Feature Engineering\n",
    "\n",
    "  ### Age in bins:\n",
    "   Â· Let's split the ages in bins: Under 30, under 40, under 50 and above 50 (There are not that meany people above 50 years old).\n",
    "    \n",
    "  ### Sex and Marriage status:\n",
    "   Â· Let's also create a new column with a combination of SEX and MARRIAGE, that will have the following structure:\n",
    "\n",
    "|**Value**|**New Feature**|\n",
    "|----|----|\n",
    "|**1**:| Married Man|\n",
    "|**2**:| Sinlge Man|\n",
    "|**3**:| Divorced Man|\n",
    "|**4**:| \"Others\" Man|\n",
    "|**5**:| Married Woman|\n",
    "|**6**:| Single Woman|\n",
    "|**7**:| Divorced Woman|\n",
    "|**8**:| \"Others\" Woman|\n",
    "\n",
    "  ### Percentage of debt agains credit limit.\n",
    "   Â· Finally, I would create a new column that will represent the percentage that the debt (*BILL_AMT1*) represents the Limit Balance (*LIMIT_BAL*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE BINS:\n",
    "new_credit['AgeBin'] = 0 #to create a column of 0\n",
    "new_credit.loc[(new_credit['AGE'] < 30) , 'AgeBin'] = 20\n",
    "new_credit.loc[((new_credit['AGE'] >= 30) & (new_credit['AGE'] < 40)) , 'AgeBin'] = 30\n",
    "new_credit.loc[((new_credit['AGE'] >= 40) & (new_credit['AGE'] < 50)) , 'AgeBin'] = 40\n",
    "new_credit.loc[(new_credit['AGE'] >= 50) , 'AgeBin'] = 50\n",
    "new_credit.AgeBin.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = new_credit.iloc[:,[1,2,3,4,25,6,12]] # First we'll test with Age\n",
    "depVar2 = new_credit.iloc[:,24]\n",
    "\n",
    "# Splitting Training and Test Sets:\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features2, depVar2, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model 2\n",
    "modelRF2 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
    "            oob_score=False, random_state=2018, verbose=False,\n",
    "            warm_start=False)\n",
    "modelRF2.fit(X_train2,y_train2)\n",
    "print(cross_val_score(modelRF2, X_train2, y_train2),\"\\n\", modelRF2.score(X_train2,y_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF2, X_test2, y_test2),\"\\n\", modelRF2.score(X_test2,y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEX and MARRIAGE:\n",
    "new_credit['M_S'] = 0 #to create a column of 0\n",
    "new_credit.loc[((new_credit['SEX'] == 1) & (new_credit['MARRIAGE'] == 1)) , 'M_S'] = 1\n",
    "new_credit.loc[((new_credit['SEX'] == 1) & (new_credit['MARRIAGE'] == 2)) , 'M_S'] = 2\n",
    "new_credit.loc[((new_credit['SEX'] == 1) & (new_credit['MARRIAGE'] == 3)) , 'M_S'] = 3\n",
    "new_credit.loc[((new_credit['SEX'] == 1) & (new_credit['MARRIAGE'] == 0)) , 'M_S'] = 4\n",
    "new_credit.loc[((new_credit['SEX'] == 2) & (new_credit['MARRIAGE'] == 1)) , 'M_S'] = 5\n",
    "new_credit.loc[((new_credit['SEX'] == 2) & (new_credit['MARRIAGE'] == 2)) , 'M_S'] = 6\n",
    "new_credit.loc[((new_credit['SEX'] == 2) & (new_credit['MARRIAGE'] == 3)) , 'M_S'] = 7\n",
    "new_credit.loc[((new_credit['SEX'] == 2) & (new_credit['MARRIAGE'] == 0)) , 'M_S'] = 8\n",
    "new_credit.M_S.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = new_credit.iloc[:,[1,26,3,5,6,12]] # Now SEX and MARRIAGE\n",
    "depVar3 = new_credit.iloc[:,24]\n",
    "\n",
    "# Splitting Training and Test Sets:\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(features3, depVar3, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model 3\n",
    "modelRF3 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
    "            oob_score=False, random_state=2018, verbose=False,\n",
    "            warm_start=False)\n",
    "modelRF3.fit(X_train3,y_train3)\n",
    "print(cross_val_score(modelRF3, X_train3, y_train3),\"\\n\", modelRF3.score(X_train3,y_train3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF3, X_test3, y_test3),\"\\n\", modelRF3.score(X_test3,y_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of limit:\n",
    "new_credit['PERCENT_LIMIT'] = (new_credit.BILL_AMT1/new_credit.LIMIT_BAL)*100\n",
    "\n",
    "new_credit.PERCENT_LIMIT.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features4 = new_credit.iloc[:,[2,3,4,5,6,27]] #Now test the \"PERCENT_LIMIT\" feature\n",
    "depVar4 = new_credit.iloc[:,24]\n",
    "\n",
    "# Splitting Training and Test Sets:\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(features4, depVar4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model 4\n",
    "modelRF4 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
    "            oob_score=False, random_state=2018, verbose=False,\n",
    "            warm_start=False)\n",
    "modelRF4.fit(X_train4,y_train4)\n",
    "print(cross_val_score(modelRF4, X_train4, y_train4),\"\\n\", modelRF4.score(X_train4,y_train4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF4, X_test4, y_test4),\"\\n\", modelRF4.score(X_test4,y_test4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features5 = new_credit.iloc[:,[3,6,25,26,27]] #Finally, all together\n",
    "depVar5 = new_credit.iloc[:,24]\n",
    "\n",
    "# Splitting Training and Test Sets:\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(features5, depVar5, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Model 5\n",
    "modelRF5 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=4,\n",
    "            oob_score=False, random_state=2018, verbose=False,\n",
    "            warm_start=False)\n",
    "modelRF5.fit(X_train5,y_train5)\n",
    "print(cross_val_score(modelRF5, X_train5, y_train5),\"\\n\", modelRF5.score(X_train5,y_train5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF5, X_test5, y_test5),\"\\n\", modelRF5.score(X_test5,y_test5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few conclusions based on the Feature Engineering.\n",
    "For this first analysis, I've realized that my Feature Enginering effort were useless. But we still need to decide if we'll upsample or downsample, as the majority of the dataset tend to go to the \"NO\"; so the results that I'm getting are completely unrealistic.\n",
    "\n",
    "In order:\n",
    "\n",
    "    Â· Age in bins: It seems that it hasn't had the desired results, as the predictions were a bit poorer than the original.\n",
    "    Â· Sex and Marriage: It's completely useless, because I haven't actually add any new information to the model. The model was able to find the same information itself by combining those columns by itself.\n",
    "    Â· Percentual Limit: This hasn't been useful neither, but I still think that it can be useful in the future, once we downsample or upsample the dataframe.\n",
    "\n",
    " ## UPSAMPLING\n",
    "Lets try to upsamplesample the minority class to try to avoid the bias of the model to go to the \"NO\" while predicting the Default Payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "new_train = X_train.join(y_train)\n",
    "\n",
    "new_majority = new_train[new_train.default_payment==\"No\"]\n",
    "new_minority = new_train[new_train.default_payment==\"Yes\"]\n",
    "\n",
    "print(new_train.default_payment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling\n",
    "new_minority_up = resample(new_minority,\n",
    "                           replace=True,     # sample with replacement\n",
    "                           n_samples=len(new_majority),    # to match majority class\n",
    "                           random_state=587)\n",
    "\n",
    "new_df_up = pd.concat([new_majority, new_minority_up])\n",
    "# Check if Majority now matches with Minority\n",
    "new_df_up.default_payment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to split the dataframe again (Dependant and independant variables):\n",
    "X_train_up = new_df_up.iloc[:,0:7]\n",
    "y_train_up = new_df_up[\"default_payment\"]\n",
    "\n",
    "# Now do some Preprocessing (Center and Scale):\n",
    "X_scaled_up = preprocessing.scale(X_train_up)\n",
    "X_scaled_up.mean(axis=0)\n",
    "X_scaled_up.std(axis=0)\n",
    "\n",
    "# Let's find out what happened with our predictions.\n",
    "modelRF.fit(X_scaled_up,y_train_up)\n",
    "print(cross_val_score(modelRF, X_scaled_up, y_train_up),\"\\n\", modelRF.score(X_scaled_up,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF, X_test, y_test),\"\\n\", modelRF.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Upsampling results:\n",
    " By upsampling, we can see that our model has imporved quite a few at the \"training set\", but this winnin might be due to overfitting the model. If we apply this model to the \"test set\", that is completely unseen by the model, the performance for the model decreases significantly. This indicates that the model may be overfitted as we suspected.\n",
    " \n",
    " ## DOWNSAMPLING\n",
    " Let's try now to do the opposite. By downsampling the trainset, we will reduce the number of the majority class up to the minority class. This process avoids the problem of overfitting; but by reducing the sample, we are exposed to loss some relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "new_majority_down = resample(new_majority,\n",
    "                           replace=False,     # sample WITHOUT replacement\n",
    "                           n_samples=len(new_minority),    # to match minority class\n",
    "                           random_state=587)\n",
    "\n",
    "new_df_down = pd.concat([new_minority, new_majority_down])\n",
    "# Check if Majority now matches with Minority\n",
    "new_df_down.default_payment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to split the dataframe again (Dependant and independant variables):\n",
    "X_train_down = new_df_down.iloc[:,0:7]\n",
    "y_train_down = new_df_down[\"default_payment\"]\n",
    "# Let's find out what happened with our predictions.\n",
    "\n",
    "modelRF.fit(X_train_down,y_train_down)\n",
    "print(cross_val_score(modelRF, X_train_down, y_train_down),\"\\n\", modelRF.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(modelRF, X_test, y_test),\"\\n\", modelRF.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling results:\n",
    "The downsampling results were definitely not the desirables, and the performance of the model has been quite poor.\n",
    "I will need to go back to the feature engineering, or maybe to select different variables that might help the model to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
